{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmWvvCOhqWmtrpNxb1rzXA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mabela1/Similarity-Classifier/blob/main/Similarity_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import dependecies"
      ],
      "metadata": {
        "id": "9Ge7_-sw3EVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import string\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import tokenize"
      ],
      "metadata": {
        "id": "jty8riNq3HgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = open('Commercial_Security_System_Market.pdf','rb')"
      ],
      "metadata": {
        "id": "ukEAQcTV3Hi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_extraction(pdf):\n",
        "    pdfReader = PyPDF2.PdfFileReader(pdf)\n",
        "    count = pdfReader.numPages\n",
        "    output = ''\n",
        "\n",
        "    for i in range(count):\n",
        "        page = pdfReader.getPage(i)\n",
        "        output += (page.extractText())\n",
        "    return output"
      ],
      "metadata": {
        "id": "t0WyGUGr3HlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text_extraction(pdf)"
      ],
      "metadata": {
        "id": "ri2ytV6Y3Hnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inputs"
      ],
      "metadata": {
        "id": "X1BnOlBJ3UG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['work','technology','nature','health','smart','security','market','economy']"
      ],
      "metadata": {
        "id": "EzS0XpqU3W6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentences(sentences):\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    clean_document_sent = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        words = word_tokenize(sent)\n",
        "        clean_sentence = []\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = lemmatizer.lemmatize(word)\n",
        "            # remove punctuation from each word\n",
        "            table = str.maketrans('','',string.punctuation)\n",
        "            word = word.translate(table)\n",
        "            if word.isalpha() == False:\n",
        "                continue\n",
        "            if word in stopWords:\n",
        "                continue\n",
        "            if (len(word)<4):\n",
        "                continue\n",
        "            clean_sentence.append(word)\n",
        "        clean_document_sent.append(clean_sentence)\n",
        "    return clean_document_sent"
      ],
      "metadata": {
        "id": "NSEyoo9r3W9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_score(doc_word_clean):\n",
        "    tf_score = {}\n",
        "    for each_word in doc_word_clean:\n",
        "        if each_word in tf_score:\n",
        "            tf_score[each_word] += 1\n",
        "        else:\n",
        "            tf_score[each_word] = 1\n",
        "\n",
        "    # Dividing by total_word_length for each dictionary element\n",
        "    total_word_length = len(doc_word_clean)\n",
        "    tf_score.update((x, y/int(total_word_length)) for x, y in tf_score.items())\n",
        "    return tf_score"
      ],
      "metadata": {
        "id": "cI0isIlG3XAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_sent(word, sentences): \n",
        "    final = [all([w in x for w in word]) for x in sentences] \n",
        "    sent_len = [sentences[i] for i in range(0, len(final)) if final[i]]\n",
        "    return int(len(sent_len))"
      ],
      "metadata": {
        "id": "_psTdUky3XDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def idf_calc_score(doc_word_clean,transform_sentences):\n",
        "    idf_score = {}\n",
        "    for each_word in doc_word_clean:\n",
        "        each_word = each_word.replace('.','')\n",
        "        if each_word in idf_score:\n",
        "            idf_score[each_word] = check_sent(each_word, transform_sentences)\n",
        "        else:\n",
        "            idf_score[each_word] = 1\n",
        "    \n",
        "    # Performing a log and divide\n",
        "    total_sent_len = len(transform_sentences)\n",
        "    idf_score.update((x, math.log(int(total_sent_len)/y)) for x, y in idf_score.items())\n",
        "    return idf_score"
      ],
      "metadata": {
        "id": "SvLtQWSO3m6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select top key words\n",
        "from operator import itemgetter\n",
        "def get_top_n(dict_elem, n):\n",
        "    result = dict(sorted(dict_elem.items(), key = itemgetter(1), reverse = True)[:n]) \n",
        "    return result"
      ],
      "metadata": {
        "id": "hjjxcjtu3m-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model_bert = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
        "def vector_pdf_embedding(key_words_new,categories):\n",
        "    # calculate weights for key words\n",
        "    key_words_weights = np.array(list(key_words_new.values()))/np.sum(np.array(list(key_words_new.values())))\n",
        "    # generate embeddings for each key word from top n list\n",
        "    embeddings_key_words = model_bert.encode(list(key_words_new.keys()))\n",
        "    \n",
        "    # generate embeddings for each category\n",
        "    embeddings_categories = model_bert.encode(categories)\n",
        "\n",
        "    # generate a embedding vector for pdf based on top key words\n",
        "    vector_pdf = np.dot(key_words_weights,embeddings_key_words)\n",
        "    return vector_pdf,embeddings_categories"
      ],
      "metadata": {
        "id": "Q5NWTGN_3nBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the Model"
      ],
      "metadata": {
        "id": "oBHLBwU33zqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_classifier(text,nr_key_words,categories):\n",
        "    # clean text\n",
        "    clean_document_sent = clean_sentences(tokenize.sent_tokenize(text))\n",
        "    clean_document_word = [item for sublist in clean_document_sent for item in sublist]\n",
        "    \n",
        "    #calculate tf_score\n",
        "    tf = tf_score(clean_document_word)\n",
        "    \n",
        "    # tranform sentences before calling idf function\n",
        "    transform_sentences = []\n",
        "    for i in clean_document_sent:\n",
        "        transform_sentences.append(' '.join(i))\n",
        "    # calculate idf_score\n",
        "    idf = idf_calc_score(clean_document_word,transform_sentences)\n",
        "    \n",
        "    #calculate tf-idf_score\n",
        "    tf_idf = {key: tf[key] * idf.get(key, 0) for key in tf.keys()}\n",
        "    \n",
        "    #extract most important words from the text\n",
        "    key_words_new = get_top_n(tf_idf, nr_key_words)\n",
        "    \n",
        "    # save pdf embeddings\n",
        "    vector_pdf,vector_category = vector_pdf_embedding(key_words_new,categories)\n",
        "    \n",
        "    #create DataFrame for categories\n",
        "    categories_df = pd.concat([pd.DataFrame(categories,columns=['category']),\n",
        "                               pd.DataFrame(vector_category)],axis=1)\n",
        "     # generate sorted list of similar categories\n",
        "    similarity_df = pd.DataFrame(((cosine_similarity(pd.DataFrame(vector_pdf).T,\n",
        "                        categories_df.iloc[:,1:])+1)/2),index = ['text'],\n",
        "                                 columns =categories_df.iloc[:,0])\n",
        "    sorted_similarity = (similarity_df.sort_values(axis=1,by='text',ascending=False)).transpose()\n",
        "    return sorted_similarity\n"
      ],
      "metadata": {
        "id": "igVZ1LmF3nFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_classifier(text,10,categories)"
      ],
      "metadata": {
        "id": "7UK7olvh3nJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}